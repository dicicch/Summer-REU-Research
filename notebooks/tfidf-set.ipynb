{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hunter\\dev\\sysfake\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import functools\n",
    "import itertools\n",
    "import pickle\n",
    "import glob\n",
    "import sys\n",
    "#from collections import deque\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, make_scorer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from waybackmachine import WaybackMachine\n",
    "\n",
    "import feature_extraction as fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 100\n",
    "CLASS_DICT = dict(zip(('real', 'fake', 'opinion', 'polarized', 'satire', 'promotional', 'correction'),\n",
    "                      (1, 2, 3, 5, 7, 9, 11)))\n",
    "features = [\"url_ending_index\", \"from_reputable_source_index\",\n",
    "            \"today_index\", \"grammar_index\", \"quotation_index\",\n",
    "            \"past_tense_index\", \"present_tense_index\", \"should_index\",\n",
    "            \"opinion_index\", \"all_caps_index\", \"from_satire_source_index\",\n",
    "            \"exclamation_index\", \"apa_index\", \"name_source_index\",\n",
    "            \"interjection_index\", \"you_index\", \"dot_gov_ending_index\",\n",
    "            \"from_unreputable_source_index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('./data/all_urls.pickle', mode='rb') as filein:\n",
    "        urls = pickle.load(filein)\n",
    "except:\n",
    "    urls = {}\n",
    "    for fname in glob.glob('./data/*urls.txt'):\n",
    "        print(fname)\n",
    "        label = fname.split('_')[0].split('\\\\')[-1]\n",
    "        urls_list = pd.read_csv(fname,\n",
    "                                delim_whitespace=True,\n",
    "                                header=None, encoding='utf-8').squeeze().to_list()\n",
    "        urls.update({label: urls_list})\n",
    "    with open('./data/all_urls.pickle', mode='wb') as fileout:\n",
    "        pickle.dump(urls, fileout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, url_list in urls.items():\n",
    "    urls.update({label: [url if 'https://' in url or 'http://' in url else 'https://'+url for url in url_list]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(f'./data/sub{sz!s}_vectors.pickle', mode='rb') as filein:\n",
    "        vectors = pickle.load(filein)\n",
    "except:\n",
    "    urls_choice = {k:np.random.choice(v, size=sz) for k, v in urls.items()}\n",
    "    vectors = {}\n",
    "    for label, url_list in urls_choice.items():\n",
    "        vector_list = [fe.ArticleVector(url=url)\n",
    "                       for url in tqdm(url_list, desc=label)]\n",
    "        vectors.update({label: vector_list})\n",
    "    with open(f'./data/sub{sz!s}_vectors.pickle', mode='wb') as fileout:\n",
    "        pickle.dump(obj=vectors, file=fileout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = {}\n",
    "for label, vector_list in vectors.items():\n",
    "    text_list = [vector.text for vector in vector_list]\n",
    "    texts.update({label: text_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical = pd.DataFrame([v.vector for v in list(itertools.chain.from_iterable(vectors.values()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_labelled = pd.melt(pd.DataFrame.from_dict(texts),\n",
    "                         var_name='label', value_name='text')\n",
    "texts_labelled = texts_labelled[texts_labelled['text']!='']\n",
    "texts_labelled = texts_labelled.replace({'label': CLASS_DICT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: huge hack\n",
    "c1 = canonical[canonical.index.isin(texts_labelled[texts_labelled['text']!=''].index)].reset_index(drop=True)\n",
    "c1.columns = features\n",
    "c1['label'] = texts_labelled[texts_labelled['text']!='']['label'].reset_index(drop=True)\n",
    "c1.to_csv('data/sub100_vectorrep_labeled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./data/texts_labelled_sub{sz!s}.pickle\", mode='wb') as fileout:\n",
    "    pickle.dump(obj=texts_labelled, file=fileout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "blocks[0,:] has incompatible row dimensions. Got blocks[0,1].shape[0] == 700, expected 654.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-34c33c0e6298>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtfidf_rep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts_labelled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtfidf_can\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_rep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtexts_labelled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/tfidf_rep.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfileout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ds\\lib\\site-packages\\scipy\\sparse\\construct.py\u001b[0m in \u001b[0;36mhstack\u001b[1;34m(blocks, format, dtype)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m     \"\"\"\n\u001b[1;32m--> 467\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ds\\lib\\site-packages\\scipy\\sparse\\construct.py\u001b[0m in \u001b[0;36mbmat\u001b[1;34m(blocks, format, dtype)\u001b[0m\n\u001b[0;32m    586\u001b[0m                                                     \u001b[0mexp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbrow_lengths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m                                                     got=A.shape[0]))\n\u001b[1;32m--> 588\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbcol_lengths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: blocks[0,:] has incompatible row dimensions. Got blocks[0,1].shape[0] == 700, expected 654."
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_rep = tfidf_vectorizer.fit_transform(texts_labelled['text'])\n",
    "tfidf_can = scipy.sparse.hstack((tfidf_rep, c1))\n",
    "y = texts_labelled['label']\n",
    "with open('./data/tfidf_rep.pickle', mode='wb') as fileout:\n",
    "    pickle.dump(tfidf_rep, fileout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_report_confusion(y_true, y_pred):\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "    #sys.stdout.write(confusion_matrix(y_true, y_pred))\n",
    "    return precision_score(y_true, y_pred, average='micro')\n",
    "\n",
    "def train(estimator, x, y):\n",
    "    scores = cross_val_score(estimator,\n",
    "                             x, y,\n",
    "                             scoring='precision_micro',\n",
    "                             cv=StratifiedKFold(n_splits=10, shuffle=True),\n",
    "                             verbose=3, n_jobs=-1)\n",
    "    print(f\"{scores!s}\" + '\\n' + f\"Mean: {np.mean(scores)!s}\")\n",
    "    \n",
    "def train2(estimator, x, y):\n",
    "    scores = cross_val_score(estimator,\n",
    "                             x, y,\n",
    "                             scoring=make_scorer(precision_report_confusion),\n",
    "                             cv=StratifiedKFold(n_splits=10, shuffle=True),\n",
    "                             verbose=3)\n",
    "    print(f\"{scores!s}\" + '\\n' + f\"Mean: {np.mean(scores)!s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vanilla SVC\")\n",
    "print(\"TFIDF Only\")\n",
    "train2(SVC(), tfidf_rep, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vanilla NuSVC\")\n",
    "print(\"TFIDF Only\")\n",
    "train2(NuSVC(), tfidf_rep, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vanilla Logistic L2-Penalized SVM\")\n",
    "print(\"TFIDF Only\")\n",
    "train2(SGDClassifier(loss='log',\n",
    "                     penalty='l2'),\n",
    "       tfidf_rep, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cosine Similarity Logistic L2-SVM\")\n",
    "print(\"TFIDF Only\")\n",
    "pipeline_steps = [('kernel', Nystroem(kernel='cosine', n_components=100)),\n",
    "                  ('clf', SGDClassifier(loss='log', penalty='l2'))]\n",
    "pl = Pipeline(pipeline_steps)\n",
    "train2(pl, tfidf_rep, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vanilla SVC\")\n",
    "print(\"TFIDF+Explication Features\")\n",
    "train2(SVC(), tfidf_can, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vanilla Logistic L2-Penalized SVM\")\n",
    "print(\"TFIDF+Explication Features\")\n",
    "train2(SGDClassifier(loss='log', penalty='l2'), tfidf_can, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cosine Similarity Logistic L2-Penalized SVM\")\n",
    "print(\"TFIDF+Explication Features\")\n",
    "pipeline_steps = [('kernel', Nystroem(kernel='cosine', n_components=600)),\n",
    "                  ('clf', SGDClassifier(loss='log', penalty='l2'))]\n",
    "pl = Pipeline(pipeline_steps)\n",
    "train2(pl, tfidf_can, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Naive Bayes\")\n",
    "print(\"TFIDF+Explication Features\")\n",
    "train2(MultinomialNB(), tfidf_can, y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "try:\n",
    "    with open('./data/all_vectors.pickle', mode='rb') as filein:\n",
    "        vectors = pickle.load(filein)\n",
    "except:\n",
    "    with open('./data/corrections_ArticleVectors.pickle', mode='rb') as filein:\n",
    "        vectors = pickle.load(filein)\n",
    "    for label, url_list in urls.items():\n",
    "        if label=='corrections':\n",
    "            continue\n",
    "        vector_list = [fe.ArticleVector(url=url)\n",
    "                       for url in tqdm(url_list, desc=label)]\n",
    "        vectors.update({label: vector_list})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vectors = {}\n",
    "for label, url_list in urls.items():\n",
    "    #try:\n",
    "    with multiprocessing.Pool(processes=8) as p:\n",
    "        #for x in p.imap(func, [1,5,3]):\n",
    "        #        sys.stdout.flush()\n",
    "        #        sys.stdout.write(\"\\r\" + label)\n",
    "        result = p.map(fe.ArticleVector, url_list)\n",
    "    #except Exception as e:\n",
    "    #    print(e)\n",
    "    #vector_list = [fe.ArticleVector(url=url)\n",
    "    #                   if url[:7]=='http://' or url[:8]=='https://'\n",
    "    #                   else fe.ArticleVector(url='https://'+url)\n",
    "    #               for url in url_list]\n",
    "    urls.update({label: vector_list})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
