{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hunter S. DiCicco\n",
    "\n",
    "with Dr. Dongwon Lee, Ph.D.\n",
    "\n",
    "# Multinomial SVC for Categories of News\n",
    "\n",
    "## Diagnostics to Assess Model Performance\n",
    "\n",
    "## TODO:\n",
    "* Refactor base modules [FIRST PRIORITY]\n",
    "* Rectify Promotional Content/Native Ads Class Balance\n",
    "* Prepare Pipeline description for misreporting\n",
    "* âœ“ Restructure repo to put URLs in the data dir and stop tracking checkpoints and caches\n",
    "* [Browser Extension Tutorial](https://www.smashingmagazine.com/2017/04/browser-extension-edge-chrome-firefox-opera-brave-vivaldi/)\n",
    "\n",
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hunter\\dev\\sysfake\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving data...\n",
      "Retrieving data...\n",
      "Retrieving data...\n",
      "Retrieving data...\n",
      "------------\n",
      "recall:[0.7  0.88 0.03 0.29 0.89]\n",
      "precision:[0.82 0.84 0.54 0.23 0.51]\n",
      "f1:[0.75 0.86 0.06 0.26 0.65]\n",
      "------------\n",
      "\n",
      "{1: 68, 2: 28, 3: 217, 5: 158, 7: 24}\n",
      "This model got 55.8% correct || 625 out of 1120.\n",
      "False negatives for ./data/real data (1)\n",
      "{5: 55, 2: 13}\n",
      "False negatives for ./data/fake data (2)\n",
      "{5: 28}\n",
      "False negatives for ./data/opinion data (3)\n",
      "{5: 144, 2: 8, 1: 12, 7: 53}\n",
      "False negatives for ./data/polarized data (5)\n",
      "{7: 139, 3: 4, 2: 15}\n",
      "False negatives for ./data/satire data (7)\n",
      "{1: 22, 3: 2}\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pickle\n",
    "import glob\n",
    "import copy\n",
    "import pdb\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from itertools import combinations\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "\n",
    "from validator import validate\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Instead of an import, we wrap the execution of both of these vital modules within `exec`.\n",
    "    This ensures that changes are enacted on every chunk evaluation.\n",
    "    \"\"\"\n",
    "    exec(open(\"classifier.py\").read(), globals())\n",
    "    exec(open(\"validator.py\").read(), globals())\n",
    "\n",
    "main()\n",
    "random_state = 4261998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_DICT = dict(zip(('real', 'fake', 'opinion', 'polarized', 'satire', 'promotional', 'correction'),\n",
    "                      (1, 2, 3, 5, 7, 9, 11)))\n",
    "\n",
    "def load_latest_model(substr='', verbose=0):\n",
    "    \"\"\"\n",
    "    Loads and returns the latest `pickle` from the `models` directory with the user-specified substring in its name.\n",
    "    If no file is found, produce a warning for `verbose>0` and then return none.\n",
    "    \"\"\"\n",
    "    # obtain a glob of the files in the `models` directory\n",
    "    list_of_files = glob.glob(f\"models/*{substr}*.pickle\")\n",
    "\n",
    "    # obtain the most recently created model with `substr` in its name\n",
    "    # will warn if verbose, and then return none if no file is found\n",
    "    try:\n",
    "        latest_model = max(list_of_files, key=os.path.getctime)\n",
    "        # open the most recent model and load it into the namespace\n",
    "        # WARNING: only load pickle files that you trust!\n",
    "        with open(latest_model, mode=\"rb\") as filein:\n",
    "            model = pickle.load(filein)\n",
    "    except ValueError:\n",
    "        if verbose>0:\n",
    "            warnings.warn(f\"No most-recent file{' containing ' if substr else '.'}\\\"{substr}\\\"\", UserWarning)\n",
    "        return None\n",
    "\n",
    "    return model\n",
    "\n",
    "def cross_val_best(x, y, kf=None, kernel='rbf'):\n",
    "    global random_state\n",
    "\n",
    "    kf = kf if kf else KFold(n_splits=20, shuffle=True, random_state=random_state)\n",
    "    splits = kf.split(x, y)\n",
    "\n",
    "    models, recalls, precisions = {}, [], []\n",
    "\n",
    "    ### report mean macro and micro average recall per fold\n",
    "    for fold_i, (train, test) in enumerate(splits):\n",
    "        x_train, y_train = x[train], y[train]\n",
    "        x_test, y_test = x[test], y[test]\n",
    "        model = SVC(C=2.075,\n",
    "                    kernel=kernel,\n",
    "                    gamma='scale',\n",
    "                    break_ties=True,\n",
    "                    random_state=random_state).fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "        score = recall_score(y_test, y_pred, average='micro')\n",
    "        #print(f\"Per-class precision: {', '.join(list(map(str, precision_score(y_test, y_pred, average=None))))}\")\n",
    "        recalls.append(recall_score(y_test, y_pred, average=None))\n",
    "        precisions.append(precision_score(y_test, y_pred, average=None))\n",
    "        models.update({model:score})\n",
    "        validate(model, x_test, y_test);\n",
    "\n",
    "    best_model = max(models, key=models.get)\n",
    "    recalls = pd.DataFrame(recalls, columns=list(CLASS_DICT.keys()))\n",
    "    precisions = pd.DataFrame(precisions, columns=list(CLASS_DICT.keys()))\n",
    "\n",
    "    print(f\"Mean Class Recall Across {kf.n_splits!s} Folds: {*recalls.mean().round(4),!s}\")\n",
    "    print(f\"Mean Class Precision Across {kf.n_splits!s} Folds: {*precisions.mean().round(4),!s}\")\n",
    "    return best_model\n",
    "\n",
    "def search(x, y, kernel='rbf', verbose=0, cv=3, refit=False, scoring='f1_micro', p=False):\n",
    "    global random_state\n",
    "    svc = SVC(kernel=kernel, class_weight='balanced', probability=p)\n",
    "    params = {'C': sp.stats.distributions.arcsine(scale=1e4),\n",
    "              'gamma':sp.stats.distributions.expon(scale=10),\n",
    "              'break_ties':(True, False)}\n",
    "    clf = RandomizedSearchCV(estimator=svc,\n",
    "                             refit=refit,\n",
    "                             cv=cv,\n",
    "                             param_distributions=params,\n",
    "                             scoring=scoring,\n",
    "                             n_jobs=-1, verbose=verbose)\n",
    "    clf.fit(x, y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Read\n",
    "\n",
    "In the following chunk we use `glob` to iterate over the files containing `ArticleVector`s, and call `pd.read_csv` on each one.\n",
    "\n",
    "The returned `pd.DataFrame` is stored in a dictionary under its respective filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of features in use\n",
    "features = [\"url_ending_index\", \"from_reputable_source_index\",\n",
    "            \"today_index\", \"grammar_index\", \"quotation_index\",\n",
    "            \"past_tense_index\", \"present_tense_index\", \"should_index\",\n",
    "            \"opinion_index\", \"all_caps_index\", \"from_satire_source_index\",\n",
    "            \"exclamation_index\", \"apa_index\", \"name_source_index\",\n",
    "            \"interjection_index\", \"you_index\", \"dot_gov_ending_index\",\n",
    "            \"from_unreputable_source_index\", \"label\"]\n",
    "\n",
    "data_with_vars = {}\n",
    "\n",
    "read_with_params = partial(pd.read_csv,\n",
    "                           delim_whitespace=True,\n",
    "                           names=features)\n",
    "\n",
    "for file in glob.glob('data\\\\*vectors*.txt'):\n",
    "    data_with_vars[file.split('\\\\')[-1].split('.')[0]] = read_with_params(file)\n",
    "\n",
    "for var, df in data_with_vars.items():\n",
    "    df['label'] = CLASS_DICT[var.split('_')[0]]\n",
    "\n",
    "d_full = pd.concat(data_with_vars.values(), ignore_index=True)\n",
    "dummy_labels = pd.get_dummies(d_full.label)\n",
    "dummy_labels.columns = list(CLASS_DICT.keys())\n",
    "d_full = d_full.drop('label', axis=1)\n",
    "d_full = pd.concat([d_full, dummy_labels], axis=1)\n",
    "\n",
    "#fig, ax = plt.subplots(3, 6)\n",
    "#for column in d_full.columns:\n",
    "#    d_full[column].plot.density(ax=ax)\n",
    "\n",
    "#d_full['label'].plot.density(ind=list(CLASS_DICT.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = *(d_full.drop('label', axis=1),\n",
    "         d_full['label']),\n",
    "\n",
    "x, y = map(lambda j: j.to_numpy(), (x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_latest_model(substr='rscv')\n",
    "if not model:\n",
    "    grid = search(x, y,\n",
    "                  kernel='linear',\n",
    "                  scoring='f1_weighted',\n",
    "                  p=False, verbose=3)\n",
    "    \n",
    "    if grid.refit:\n",
    "        model = grid.best_estimator_\n",
    "        with open(\"models/best-model-rscv-full_kernel-{kernel}_C-{C}_breakties-{break_ties}.pickle\".format(**model.get_params()), mode='wb') as fileout:\n",
    "            pickle.dump(model, fileout)\n",
    "\n",
    "    results = pd.DataFrame(grid.cv_results_)\n",
    "    results.to_csv(f\"./figures/grid_full_{np.int(np.floor(grid.best_score_)*100)!s}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "NOTES: *Some of the insights on linear feature importance used below obtained from [here](https://stackoverflow.com/a/22831491).*\n",
    "\n",
    "*`plot_coefficients` method initially obtained from [here](https://medium.com/@aneesha/visualising-top-features-in-linear-svm-with-scikit-learn-and-matplotlib-3454ab18a14d)*\n",
    "\n",
    "In the next chunk we will use our models' learned *support vectors* to perform feature selection.\n",
    "\n",
    "Training a Support Vector machine involves building *separating hyperplanes* to differentiate data by label using the *one-vs.-rest (OVR)* strategy.\n",
    "\n",
    "Namely, say we trained an SVM on data with three labels. During inference, the SVM classifies by checking what region the new example occupies and saying \"Since it's in this region, it does not belong to class `one` nor does it belong to class `three`, so it must belong to class `two`.\" *One-Vs.-Rest* is an excellent strategy because weights are learned by abstracting a multinomial problem into several binary problems!\n",
    "\n",
    "In the case of a C-Support Vector Machine (SVC), we use a parameter called `C` to allow for limited deviation from the hyperplane that we build.\n",
    "\n",
    "This means that our hyperplanes are relaxed -- there is `C` leeway such that examples within `C` distance of the hyperplane can be assumed to belong to the other side. Generally, the larger `C` is, the more the error function will punish misclassifications -- as `C` tends to infinity the model acts like a hard margin model instead.\n",
    "\n",
    "When we are done, each feature is assigned an associated *support vector*, which contains *weight coefficients*. These coefficients directly explain how much influence a feature has when our model performs inference.\n",
    "\n",
    "The rows of the weight matrix describe the unique feature weights for each pair in the separation problems, which can be written as the mathematical combinations of each of the class labels with the others (`list(combinations(CLASS_DICT.values(), 2))`):\n",
    "\n",
    "| hyperplane comparing | first class | second class |\n",
    "|----------------------|-------------|--------------|\n",
    "|weights for|1|2|\n",
    "|weights for|1|3|\n",
    "|...|\n",
    "|weights for|1|11|\n",
    "|weights for|2|3|\n",
    "|weights for|2|4|\n",
    "|...|\n",
    "|weights for|2|11|\n",
    "|...|\n",
    "|weights for|7|11|\n",
    "|weights for|9|11|\n",
    "\n",
    "When we take the *vector norm* of each feature-weight vector, we get a sense (**not** a direct measurement) of feature variance when in the context of the other features in the training data. This is different from taking the feature-wise variance of the data, because this measurement speaks directly to the efficacy of a feature's variance for a specific separation problem.\n",
    "\n",
    "One final area of examination would be the model's trained *dual coefficients*, which are out of the author's scope and would require further research.\n",
    "\n",
    "[We may be able to use this to create a similar dual visualization.](https://cel.archives-ouvertes.fr/cel-01003007/file/Lecture2_Linear_SVM_Dual.pdf)\n",
    "\n",
    "**In summary we should prune those features which have near-zero contribution according to the weights, as well as those that have very little overall feature-weight variance.**\n",
    "\n",
    "Viewing the weight coefficients superimposed on each other in feature space as well as the overall feature-weight variances would give us a great idea as to which features contribute to which inference results. Let's build those visualizations now:\n",
    "\n",
    "6/18: The chunks below are broken for the moment as there are no good linear models from our `RandomizedSearchCV` yet."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "features = [\"url_ending_index\", \"from_reputable_source_index\",\n",
    "            \"today_index\", \"grammar_index\", \"quotation_index\",\n",
    "            \"past_tense_index\", \"present_tense_index\", \"should_index\",\n",
    "            \"opinion_index\", \"all_caps_index\", \"from_satire_source_index\",\n",
    "            \"exclamation_index\", \"apa_index\", \"name_source_index\",\n",
    "            \"interjection_index\", \"you_index\", \"dot_gov_ending_index\",\n",
    "            \"from_unreputable_source_index\"]\n",
    "\n",
    "# we are only writing procedures for linear (done) and rbf (todo)\n",
    "# linear has easy-to-interpret feature-weight calculations\n",
    "# and rbf uses the dual coefficients, which are notoriously harder to interpret\n",
    "\n",
    "def plot_coefficients(classifier, feature_names):\n",
    "    \"\"\"\n",
    "    Method Docstring Placeholder\n",
    "    \"\"\"\n",
    "    #pdb.set_trace()\n",
    "    global CLASS_DICT\n",
    "\n",
    "    # extract class labels and #classes\n",
    "    classes = list(CLASS_DICT.keys())\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    # use itertools.combinations to create a list\n",
    "    # of the pair combinations of class labels\n",
    "    classes = list(combinations(classes, 2))\n",
    "\n",
    "    # coerce the weight coefficient matrix to a pandas dataframe\n",
    "    # for utility\n",
    "    if classifier.kernel=='linear':\n",
    "        coef = pd.DataFrame(classifier.coef_, columns=feature_names)\n",
    "    else: \n",
    "        coef = pd.DataFrame(classifier.support_vectors_, columns=feature_names)\n",
    "    # we must transpose `coef` to plot it in feature space\n",
    "    ax = coef.T.plot(kind=\"bar\",\n",
    "                     stacked=True,\n",
    "                     figsize=(16,8),\n",
    "                     colormap=\"viridis\",\n",
    "                     title=f\"for SVC with {classifier.kernel} kernel and gamma={classifier.gamma!s}\")\n",
    "\n",
    "    # now we need a handle for the plot's legend\n",
    "    # in order to correctly label the separation problems\n",
    "    legend = ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    # the i'th patch label in the legend\n",
    "    # should contain the items of the combination pairs\n",
    "    for i, label in enumerate(legend.get_texts()):\n",
    "        label.set_text(f\"{classes[i][0]!s} vs {classes[i][1]!s}\")\n",
    "\n",
    "    # a line at the origin to create a visual anchor\n",
    "    plt.axhline(linestyle='dashed', color='firebrick')\n",
    "\n",
    "    # setting axis labels, over-title and saving the figure\n",
    "    ax.set(xlabel=\"Feature\", ylabel=\"Support Coefficient\")\n",
    "    plt.suptitle(\"Per-Problem Feature Support Coefficients\")\n",
    "    plt.savefig(f\"./figures/{len(CLASS_DICT)!s}lab_featureselection_1.png\")\n",
    "    return coef\n",
    "\n",
    "coef = plot_coefficients(model, features)\n",
    "\n",
    "# obtain the number of classes\n",
    "# as well as weights for supports of each class\n",
    "(n_classes, coefs) = *(len(model.classes_), model.coef_.transpose()),\n",
    "\n",
    "# take the vector norm ([x dot x]^(1/2)) of each feature-weight vector (axis 1)\n",
    "dot_coefs = np.apply_along_axis(lambda x: np.sqrt(x.dot(x)), 1, coefs)\n",
    "\n",
    "# subset features with very low contributions\n",
    "# in a list of features we should ablate\n",
    "features_to_ablate = list(coef.mean()[dot_coefs <= np.mean(dot_coefs)].index)\n",
    "\n",
    "### plotting\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "# using seaborn for utility\n",
    "ax = sns.barplot(features,\n",
    "                 dot_coefs,\n",
    "                 palette=sns.color_palette(\"cubehelix\", len(features)))\n",
    "\n",
    "ax.set_title(f\"for SVC with {model.kernel} kernel and gamma={model.gamma!s}\")\n",
    "plt.suptitle(\"Overall Feature Variance\")\n",
    "ax.set_xticklabels(features, rotation=90)\n",
    "ax.set(xlabel='Feature', ylabel='Variance Magnitude')\n",
    "plt.savefig(f\"./figures/{len(CLASS_DICT)!s}lab_featureselection_2.png\")\n",
    "###\n",
    "\n",
    "# each vector in the list below represents the support vectors for \n",
    "#coefs = model.dual_coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset of Robust-Feature Data\n",
    "\n",
    "#### Hand Selection Based on Visualization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# use only the most informative columns\n",
    "# or, drop the uninformative ones\n",
    "d_subset_robust = d_full.drop(features_to_ablate, axis=1)\n",
    "\n",
    "# eliminate duplicate rows\n",
    "d_subset_robust = d_subset_robust[~d_subset_robust.duplicated()]\n",
    "\n",
    "#y = d_full[list(CLASS_DICT.keys())].to_numpy()\n",
    "#x = d_full[d_full.columns.difference(list(CLASS_DICT.keys()))].to_numpy()\n",
    "\n",
    "# x: all but the label column (features)\n",
    "# y: only the label column (target)\n",
    "x, y = *(d_subset_robust[d_subset_robust.columns.difference(['label'])],\n",
    "         d_subset_robust['label']),\n",
    "\n",
    "x, y = map(lambda j: j.to_numpy(), (x,y))\n",
    "\n",
    "best_model = cross_val_best(x, y)\n",
    "\n",
    "with open(f\"models/best-model_kernel[{best_model.kernel}]gamma[{best_model.gamma}]_subset.pickle\", mode=\"wb\") as fileout:\n",
    "    pickle.dump(best_model, fileout)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "g = sns.stripplot(x='variable',\n",
    "                  y='value',\n",
    "                  hue='label',\n",
    "                  data=pd.melt(d_subset_robust,\n",
    "                               id_vars='label',\n",
    "                               value_vars=d_subset_robust.columns.drop('label')))\n",
    "g.set_xticklabels(d_subset_robust.columns, rotation=90)\n",
    "g.set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "perps = np.arange(55, 100, 5)\n",
    "try:\n",
    "    embeds_list = glob.glob(\"./data/embeds_list*.pickle\")[0]\n",
    "    with open(embeds_list, mode='rb') as filein:\n",
    "        embeds = pickle.load(filein)\n",
    "except:\n",
    "    # fit on the fly\n",
    "    kl_divs, embeds = [], []\n",
    "    for perplexity in perps:\n",
    "        print(\"Fitting new TSNE...\")\n",
    "        ts = TSNE(init='random',\n",
    "                  perplexity=perplexity,\n",
    "                  random_state=random_state,\n",
    "                  verbose=1, # recall: all output --> stdout, so it will render in command line\n",
    "                  n_jobs=-1)\n",
    "        embedded = ts.fit_transform(x)\n",
    "        embeds.append(embedded)\n",
    "        kl_divs.append(ts.kl_divergence_)\n",
    "        print(f\"Perplexity: {perplexity!s} | KLD: {np.round(ts.kl_divergence_, 3)!s}\")\n",
    "    scored_embeds = dict(zip(range(len(embeds)), kl_divs))\n",
    "    scored_embeds = sorted(scored_embeds, key=scored_embeds.get)\n",
    "    with open(f\"./data/embeds_list_{'-'.join(list(map(str, perps)))}.pickle\", mode='wb') as fileout:\n",
    "        pickle.dump(embeds, fileout)\n",
    "#fits, kl_divs = list(scored_embeds.keys()), list(scored_embeds.values())\n",
    "#with open(f\"./data/d-subset_TSNE-{best_ts.n_components!s}_init={best_ts.init}_perplexity={best_ts.perplexity!s}_random_state={random_state!s}.pickle\", mode='wb') as fileout:\n",
    "#    pickle.dump(embedded, fileout)\n",
    "\n",
    "# convert to a dataframe if we stored the embeddings as an `ndarray`\n",
    "#if not isinstance(embedded, pd.DataFrame) and isinstance(embedded, np.ndarray):\n",
    "#    xe, ye = embedded[:,0], embedded[:,1]\n",
    "#    embedded = pd.DataFrame({'x':xe, 'y':ye, 'label':d_subset_robust['label']})\n",
    "\n",
    "# we need a mapping of numeric label to string label\n",
    "# in a moment we will use it to assemble the plot's legend\n",
    "\n",
    "fig, ax = plt.subplots(3, 3, figsize=(18,16), dpi=50)\n",
    "\n",
    "for i, axrow in enumerate(ax):\n",
    "    for j, subplot in enumerate(axrow):\n",
    "        embedded = embeds[3*i + j] \n",
    "        xe, ye = embedded[:,0], embedded[:,1]\n",
    "        subplot_ax = sns.scatterplot(xe, ye,\n",
    "                                     hue=d_subset_robust['label'],\n",
    "                                     ax=subplot, legend='full')\n",
    "        handles, _ = subplot_ax.get_legend_handles_labels()\n",
    "        subplot_ax.get_legend().remove()\n",
    "        subplot_ax.set_title(f\"Perplexity: {perps[3*i + j]!s}\")\n",
    "        subplot_ax.set_facecolor('xkcd:powder blue')\n",
    "\n",
    "plt.figlegend(handles, ['label']+list(CLASS_DICT.keys()), loc='center right', facecolor='xkcd:blue/grey')\n",
    "\n",
    "ax = fig.add_subplot(111, frameon=False)\n",
    "plt.tick_params(labelcolor='none', top='off', bottom='off', left='off', right='off')\n",
    "\n",
    "plt.suptitle(\"Randomly-initialized TSNE 2-Embeddings of Article Vectors\", fontsize=24);\n",
    "ax.set_xlabel(\"First Component\", fontsize=18);\n",
    "ax.set_ylabel(\"Second Component\", fontsize=18);\n",
    "plt.savefig(\"./figures/TSNE_grid.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
