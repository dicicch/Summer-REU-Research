{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Set\n",
    "\n",
    "Initial work found in [this BERT Sentence Embedding tutorial](https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#sentence-vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from sklearn.metrics import classification_report, make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marked_text(input_sent):\n",
    "    return \"[CLS] \" + input_sent + \" [SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../data/texts_labelled_sub100.pickle\", mode='rb') as filein:\n",
    "    texts = pickle.load(filein)\n",
    "\n",
    "texts = texts[texts['text'] != '']\n",
    "\n",
    "with open(f\"../data/all_urls.pickle\", mode='rb') as filein:\n",
    "    urls = pickle.load(filein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, be sure to initialize `BertModel` with `output_hidden_states=True`\n",
    "\n",
    "For each article:\n",
    "* We call `nltk.sent_tokenize` on its scraped text,\n",
    "* then we sandwich each sentence above between the `[CLS]` and `[SEP]` tags\n",
    "* then we call `BertTokenizer.tokenize` on each sentence of the above,\n",
    "* then we call `BertTokenizer.convert_tokens_to_ids` on each tokenized text above forming `indexed_tokens`,\n",
    "* then we assign a segment ID of `1` to all of the tokens in each individual sentence, forming `segment_ids`\n",
    "    * since we will be passing each sentence through without the objective of comparing them (they all come from the same document)\n",
    "* then we convert both `indexed_tokens` and `segment_ids` into `torch.tensor`s\n",
    "    * `x = torch.tensor([x])`\n",
    "* then, for each pair of tokenized text and segment ID array, we run it through a forward pass in the pretrained `BertModel`\n",
    "* then for each hidden state array extract the second to last item and average it on the vertical axis to form a 768-element \"sentence embedding\"\n",
    "* finally, average all of the sentence embeddings for a particular article to form the \"document embedding\"\n",
    "* when we have the full collection of document embeddings, we convert them using [this formula](https://stackoverflow.com/a/57942576):\n",
    "    * first copy them all to the CPU\n",
    "    * then convert them all to `numpy` arrays\n",
    "    * finally construct a `pandas` DataFrame out of the above list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "700it [08:27,  1.38it/s]\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for (_, row) in tqdm(texts.iterrows(), total=texts.shape[0]):\n",
    "    if not row['text']:\n",
    "        continue\n",
    "    sents = nltk.sent_tokenize(row['text'])\n",
    "    marked_sents = [marked_text(sentence) for sentence in sents]\n",
    "\n",
    "    tokenized_text = [tokenizer.tokenize(sentence) for sentence in marked_sents]\n",
    "    segments_ids = [[1] * len(sentence) for sentence in tokenized_text]\n",
    "    indexed_tokens = [tokenizer.convert_tokens_to_ids(sentence) for sentence in tokenized_text]\n",
    "\n",
    "    tokens_tensors = [torch.tensor([sentence], device=device) for sentence in indexed_tokens]\n",
    "    segments_tensors = [torch.tensor([sentence], device=device) for sentence in segments_ids]\n",
    "    sent_embeddings = []\n",
    "    for sentence, segments in tuple(zip(tokens_tensors, segments_tensors)):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(sentence, segments)\n",
    "        hidden_states = outputs[2]\n",
    "        token_vecs = hidden_states[-2][0]\n",
    "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "        sent_embeddings.append(sentence_embedding)\n",
    "    document = torch.stack(sent_embeddings)\n",
    "    documents.append(torch.mean(document, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_cpu = [embedding.cpu().numpy() for embedding in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_df = pd.DataFrame(documents_cpu, dtype='object')\n",
    "bert_df['label'] = texts['label']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
