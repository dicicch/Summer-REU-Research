{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Set\n",
    "\n",
    "Initial work found in [this BERT Sentence Embedding tutorial](https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#sentence-vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "def marked_text(input_sent):\n",
    "    return \"[CLS]\" + input_sent + \"[SEP]\"\n",
    "\n",
    "#token_vecs = hidden_states[-2][0]\n",
    "#sentence_embedding = torch.mean(token_vecs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../data/texts_labelled_sub100.pickle\", mode='rb') as filein:\n",
    "    texts = pickle.load(filein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True,\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, be sure to initialize `BertModel` with `output_hidden_states=True`\n",
    "\n",
    "For each article:\n",
    "* We call `nltk.sent_tokenize` on its scraped text,\n",
    "* then we sandwich each sentence above between the `[CLS]` and `[SEP]` tags\n",
    "* then we call `BertTokenizer.tokenize` on each sentence of the above,\n",
    "* then we call `BertTokenizer.convert_tokens_to_ids` on each tokenized text above forming `indexed_tokens`,\n",
    "* then we assign a segment ID of `1` to all of the tokens in each individual sentence, forming `segment_ids`\n",
    "    * since we will be passing each sentence through without the objective of comparing them (they all come from the same document)\n",
    "* then we convert both `indexed_tokens` and `segment_ids` into `torch.tensor`s\n",
    "    * `x = torch.tensor([x])`\n",
    "* then, for each pair of tokenized text and segment ID array, we run it through a forward pass in the pretrained `BertModel` making sure to initialize it with `output_hidden_states=True`\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_text = tokenizer.tokenize(marked_text)\n",
    "#indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "#\n",
    "#tokens_tensor = torch.tensor([indexed_tokens])\n",
    "#segments_ids = [1] * len(tokenized_text)\n",
    "#segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with torch.no_grad():\n",
    "#\n",
    "#    outputs = model(tokens_tensor, segments_tensors)\n",
    "#\n",
    "#    # Evaluating the model will return a different number of objects based on \n",
    "#    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "#    # because we set `output_hidden_states = True`, the third item will be the \n",
    "#    # hidden states from all layers. See the documentation for more details:\n",
    "#    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "#    hidden_states = outputs[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
