{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hunter S. DiCicco\n",
    "\n",
    "with Dr. Dongwon Lee, Ph.D.\n",
    "\n",
    "# Data Collection: Limited Web Scraping\n",
    "for *Promotional* and *Misreporting* Content\n",
    "\n",
    "## TODO:\n",
    "* Rectify Promotional Content/Native Ads Class Balance\n",
    "\n",
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import requests as r\n",
    "import itertools as it\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "os.chdir('../')\n",
    "import feature_extraction as fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = r.utils.default_headers()\n",
    "headers.update({'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Site</th>\n",
       "      <th>Status</th>\n",
       "      <th>Coder</th>\n",
       "      <th>Story 1</th>\n",
       "      <th>Story 2</th>\n",
       "      <th>Story 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yahoo News</td>\n",
       "      <td>DONE</td>\n",
       "      <td>DI</td>\n",
       "      <td>https://goingthere.yahoo.com/post/133803688011...</td>\n",
       "      <td>https://goingthere.yahoo.com/post/130087715036...</td>\n",
       "      <td>https://goingthere.yahoo.com/post/130160620511...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABC News \\n</td>\n",
       "      <td>Done</td>\n",
       "      <td>DI</td>\n",
       "      <td>http://abcnews.go.com/US/wireStory/providence-...</td>\n",
       "      <td>http://abcnews.go.com/Politics/obama-hits-trum...</td>\n",
       "      <td>http://abcnews.go.com/Politics/justice-antonin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNN Network</td>\n",
       "      <td>DONE</td>\n",
       "      <td>BWW</td>\n",
       "      <td>http://sponsorcontent.cnn.com/interactive/cine...</td>\n",
       "      <td>http://sponsorcontent.cnn.com/amazon/article/8...</td>\n",
       "      <td>http://sponsorcontent.cnn.com/subaru/article/a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NBC News Digital</td>\n",
       "      <td>DONE</td>\n",
       "      <td>NJE</td>\n",
       "      <td>http://www.nbcnews.com/video/this-designer-cut...</td>\n",
       "      <td>http://www.nbcnews.com/video/baby-boomers-are-...</td>\n",
       "      <td>http://www.nbcnews.com/video/he-creates-furnit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HuffingtonPost.com</td>\n",
       "      <td>DONE</td>\n",
       "      <td>BWW</td>\n",
       "      <td>http://www.huffingtonpost.com/2015/06/04/best-...</td>\n",
       "      <td>http://www.huffingtonpost.com/kathy-magee/a-sm...</td>\n",
       "      <td>http://www.huffingtonpost.com/2015/05/29/evolu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Site Status Coder  \\\n",
       "0          Yahoo News   DONE    DI   \n",
       "1         ABC News \\n   Done    DI   \n",
       "2         CNN Network   DONE   BWW   \n",
       "3    NBC News Digital   DONE   NJE   \n",
       "4  HuffingtonPost.com   DONE   BWW   \n",
       "\n",
       "                                             Story 1  \\\n",
       "0  https://goingthere.yahoo.com/post/133803688011...   \n",
       "1  http://abcnews.go.com/US/wireStory/providence-...   \n",
       "2  http://sponsorcontent.cnn.com/interactive/cine...   \n",
       "3  http://www.nbcnews.com/video/this-designer-cut...   \n",
       "4  http://www.huffingtonpost.com/2015/06/04/best-...   \n",
       "\n",
       "                                             Story 2  \\\n",
       "0  https://goingthere.yahoo.com/post/130087715036...   \n",
       "1  http://abcnews.go.com/Politics/obama-hits-trum...   \n",
       "2  http://sponsorcontent.cnn.com/amazon/article/8...   \n",
       "3  http://www.nbcnews.com/video/baby-boomers-are-...   \n",
       "4  http://www.huffingtonpost.com/kathy-magee/a-sm...   \n",
       "\n",
       "                                             Story 3  \n",
       "0  https://goingthere.yahoo.com/post/130160620511...  \n",
       "1  http://abcnews.go.com/Politics/justice-antonin...  \n",
       "2  http://sponsorcontent.cnn.com/subaru/article/a...  \n",
       "3  http://www.nbcnews.com/video/he-creates-furnit...  \n",
       "4  http://www.huffingtonpost.com/2015/05/29/evolu...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nativead = pd.read_excel(\"data/Nativead.xlsx\")\n",
    "nativead.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>URLs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S. Department of State</td>\n",
       "      <td>https://www.state.gov/release-of-the-united-st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. Department of State</td>\n",
       "      <td>https://www.state.gov/conclusion-of-negotiatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U.S. Department of State</td>\n",
       "      <td>https://www.state.gov/u-s-mexico-joint-declara...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U.S. Department of State</td>\n",
       "      <td>https://www.state.gov/united-states-restricts-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S. Department of State</td>\n",
       "      <td>https://www.state.gov/president-donald-j-trump...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     source                                               URLs\n",
       "0  U.S. Department of State  https://www.state.gov/release-of-the-united-st...\n",
       "1  U.S. Department of State  https://www.state.gov/conclusion-of-negotiatio...\n",
       "2  U.S. Department of State  https://www.state.gov/u-s-mexico-joint-declara...\n",
       "3  U.S. Department of State  https://www.state.gov/united-states-restricts-...\n",
       "4  U.S. Department of State  https://www.state.gov/president-donald-j-trump..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promo = pd.read_excel(\"data/PR_content_news.xlsx\")\n",
    "promo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DONE                38\n",
       "Done                 5\n",
       "N/A, now defunct     1\n",
       "Found only 1         1\n",
       "Name: Status, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nativead['Status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Site       0\n",
       "Status     0\n",
       "Coder      0\n",
       "Story 1    0\n",
       "Story 2    0\n",
       "Story 3    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nativead = nativead.astype(str)\n",
    "nativead = nativead[nativead['Status'].apply(str.lower)=='done']\n",
    "nativead.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Site</th>\n",
       "      <th>Story 1</th>\n",
       "      <th>Story 2</th>\n",
       "      <th>Story 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yahoo News</td>\n",
       "      <td>https://goingthere.yahoo.com/post/133803688011...</td>\n",
       "      <td>https://goingthere.yahoo.com/post/130087715036...</td>\n",
       "      <td>https://goingthere.yahoo.com/post/130160620511...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABC News \\n</td>\n",
       "      <td>http://abcnews.go.com/US/wireStory/providence-...</td>\n",
       "      <td>http://abcnews.go.com/Politics/obama-hits-trum...</td>\n",
       "      <td>http://abcnews.go.com/Politics/justice-antonin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNN Network</td>\n",
       "      <td>http://sponsorcontent.cnn.com/interactive/cine...</td>\n",
       "      <td>http://sponsorcontent.cnn.com/amazon/article/8...</td>\n",
       "      <td>http://sponsorcontent.cnn.com/subaru/article/a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NBC News Digital</td>\n",
       "      <td>http://www.nbcnews.com/video/this-designer-cut...</td>\n",
       "      <td>http://www.nbcnews.com/video/baby-boomers-are-...</td>\n",
       "      <td>http://www.nbcnews.com/video/he-creates-furnit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HuffingtonPost.com</td>\n",
       "      <td>http://www.huffingtonpost.com/2015/06/04/best-...</td>\n",
       "      <td>http://www.huffingtonpost.com/kathy-magee/a-sm...</td>\n",
       "      <td>http://www.huffingtonpost.com/2015/05/29/evolu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Site                                            Story 1  \\\n",
       "0          Yahoo News  https://goingthere.yahoo.com/post/133803688011...   \n",
       "1         ABC News \\n  http://abcnews.go.com/US/wireStory/providence-...   \n",
       "2         CNN Network  http://sponsorcontent.cnn.com/interactive/cine...   \n",
       "3    NBC News Digital  http://www.nbcnews.com/video/this-designer-cut...   \n",
       "4  HuffingtonPost.com  http://www.huffingtonpost.com/2015/06/04/best-...   \n",
       "\n",
       "                                             Story 2  \\\n",
       "0  https://goingthere.yahoo.com/post/130087715036...   \n",
       "1  http://abcnews.go.com/Politics/obama-hits-trum...   \n",
       "2  http://sponsorcontent.cnn.com/amazon/article/8...   \n",
       "3  http://www.nbcnews.com/video/baby-boomers-are-...   \n",
       "4  http://www.huffingtonpost.com/kathy-magee/a-sm...   \n",
       "\n",
       "                                             Story 3  \n",
       "0  https://goingthere.yahoo.com/post/130160620511...  \n",
       "1  http://abcnews.go.com/Politics/justice-antonin...  \n",
       "2  http://sponsorcontent.cnn.com/subaru/article/a...  \n",
       "3  http://www.nbcnews.com/video/he-creates-furnit...  \n",
       "4  http://www.huffingtonpost.com/2015/05/29/evolu...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nativead = nativead.drop(['Status', 'Coder'], axis=1)\n",
    "nativead.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_url(urls=pd.DataFrame):\n",
    "    return np.random.choice(urls.to_numpy().flatten())\n",
    "\n",
    "story_urls = nativead[['Story 1', 'Story 2', 'Story 3']]\n",
    "test_url = get_article_url(story_urls)\n",
    "article = r.get(test_url, headers)\n",
    "while not article:\n",
    "    print(test_url + ' Response code 404, trying again...')\n",
    "    try:\n",
    "        test_url = get_article_url(story_urls)\n",
    "        article = r.get(test_url, headers)\n",
    "    except r.exceptions.MissingSchema:\n",
    "        continue\n",
    "else:\n",
    "    data = article.text\n",
    "    soup = BeautifulSoup(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This website uses cookies in order to enhance your experience. Please review ourPrivacy Policyto learn how we may use cookies and how you can change your browser settings to disable cookies. By continuing to use this website without changing your settings, you consent to our use of cookies.\n",
      "The World Wide Web began in 1989. Soon after was the first iteration of AOL. (Remember AOL? And for those too young to remember ... just count yourself lucky.)\n",
      "(Insert dial-up modem sound here.)\n",
      "Invented by British scientist Tim Berners-Lee, the web's goal was to help scientists share information around the world. Here's what the first webpage looked like:\n",
      "\n",
      "Snazzy, right?!\n",
      "Just four years later, it was announced that the World Wide Web (or W3, as some called it then) would be free for everyone who had Internet access. And the rest of that history ... is still being written.\n",
      "From staying in touch with our loved ones, to helping people halfway across the world who are living through tragedies, to millions of us understanding just a bit more about how other human beings live every day, to seeing firsthand what we're doing to the earth ... the web helps us all connect with our world and the beautiful creatures that reside in it.\n",
      "It's also a place where some folks want to take over your video camera and spy on you. And where people try to steal your credit card numbers. And more.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join([para.get_text(strip=True) for para in soup.find_all('p')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.upworthy.com/time-passes/',\n",
       " 'https://www.upworthy.com/more/',\n",
       " 'https://www.upworthy.com/most-shared/',\n",
       " 'https://www.upworthy.com/family/',\n",
       " 'https://www.upworthy.com/heroes/']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sponsored_div = soup.find_all('div', attrs={'class':re.compile(\"sponsor|provided\")})\n",
    "links = soup.findAll('a', attrs={'href': re.compile(\"^https*:\\/\\/\")})\n",
    "#[j.find_all('div', attrs={'class':'item'}) for j in sponsored_div if j.find_all('div', attrs={'class':'item'})!=[]]\n",
    "[j.get('href') for j in links][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    final_frame = pd.read_csv('data/promotional_text.csv')\n",
    "except:\n",
    "    texts = []\n",
    "    urls = nativead[['Story 1', 'Story 2', 'Story 3']].to_numpy().flatten().tolist() + promo['URLs'].to_list()\n",
    "    good_urls = []\n",
    "    more_urls = []\n",
    "    badresponses = 0\n",
    "    for i, url in enumerate(urls):\n",
    "        if url=='nan':\n",
    "            texts.append('')\n",
    "            continue\n",
    "        try:\n",
    "            article = r.get(url, headers)\n",
    "            if article:\n",
    "                good_urls.append(url)\n",
    "                links = soup.select('body p > a')\n",
    "                data = article.text\n",
    "                soup = BeautifulSoup(data)\n",
    "                article = \" \".join([para.get_text(strip=True) for para in soup.find_all('p')])\n",
    "                texts.append(article)\n",
    "            else:\n",
    "                badresponses += 1\n",
    "                texts.append('')\n",
    "        except r.exceptions.TooManyRedirects:\n",
    "            texts.append('')\n",
    "            continue\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"\\r\" + f\"{i!s}/{len(urls)!s} with {badresponses!s} bad responses.\")\n",
    "    sources = nativead['Site'].repeat(3).to_list() + promo['source'].to_list()\n",
    "    final_frame = pd.DataFrame({'source':sources,\n",
    "                                'url': urls,\n",
    "                                'text': texts})\n",
    "    final_frame = final_frame[final_frame['text']!='']\n",
    "    final_frame = final_frame[final_frame['text']!=' ']\n",
    "    final_frame.to_csv('data/promotional_text.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABC News \\n</td>\n",
       "      <td>http://abcnews.go.com/Politics/obama-hits-trum...</td>\n",
       "      <td> -- \\nPresidentObamasaid today that he does n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABC News \\n</td>\n",
       "      <td>http://abcnews.go.com/Politics/justice-antonin...</td>\n",
       "      <td> -- \\nEven though the relatives and doctors o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNN Network</td>\n",
       "      <td>http://sponsorcontent.cnn.com/interactive/cine...</td>\n",
       "      <td>“Everything that needs to be said hasalready b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NBC News Digital</td>\n",
       "      <td>http://www.nbcnews.com/video/this-designer-cut...</td>\n",
       "      <td>Watch live: NY Gov. Andrew Cuomo holds coronav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NBC News Digital</td>\n",
       "      <td>http://www.nbcnews.com/video/baby-boomers-are-...</td>\n",
       "      <td>Watch live: NY Gov. Andrew Cuomo holds coronav...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             source                                                url  \\\n",
       "0       ABC News \\n  http://abcnews.go.com/Politics/obama-hits-trum...   \n",
       "1       ABC News \\n  http://abcnews.go.com/Politics/justice-antonin...   \n",
       "2       CNN Network  http://sponsorcontent.cnn.com/interactive/cine...   \n",
       "3  NBC News Digital  http://www.nbcnews.com/video/this-designer-cut...   \n",
       "4  NBC News Digital  http://www.nbcnews.com/video/baby-boomers-are-...   \n",
       "\n",
       "                                                text  \n",
       "0   -- \\nPresidentObamasaid today that he does n...  \n",
       "1   -- \\nEven though the relatives and doctors o...  \n",
       "2  “Everything that needs to be said hasalready b...  \n",
       "3  Watch live: NY Gov. Andrew Cuomo holds coronav...  \n",
       "4  Watch live: NY Gov. Andrew Cuomo holds coronav...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('promotional_news_urls.txt', mode='w') as fileout:\n",
    "    fileout.write(\"\\n\".join(final_frame.url.to_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now assemble `ArticleVectors` straight-on, supplying one or both of the article text and the URL. This is demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.8 ms ± 9.16 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "x = fe.ArticleVector(text='sample text', url=test_url)\n",
    "x.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298 ms ± 21.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "x = fe.ArticleVector(url=test_url)\n",
    "x.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full disclosure: Due to limitations in the current version of `goose3` I am unable to properly handle `ReadTimeout` exceptions, which occur when an article takes longer than expected for `requests` to receive data from. I can arbitrarily extend the timeout threshold, but this of course means that each article will take even longer to extract. Instead I opt to use the text that I scraped using `BeautifulSoup` and **omit the article title** and Goose extraction altogether.\n",
    "\n",
    "This is of no consequence in the end -- the title is only used to determine a portion of an article's `all_caps_index` and `apa_index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [fe.ArticleVector(text=row['text'], url=row['url']) for _, row in final_frame.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/promotional_news_vectors.txt', mode='w') as fileout:\n",
    "    fileout.write(\"\\n\".join([str(\" \".join(list(map(str, j.vector)))) for j in vectors]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On to the NYTimes BBC Corrections dataset.\n",
    "\n",
    "I use a Powershell `cmdlet` originally obtained from [this source](http://woshub.com/powershell-get-folder-sizes/) to find those directories whose size is greater than 19.57Kb (the size of an empty Git repository). These then contain text snapshots of the first revisions of articles.\n",
    "\n",
    "```\n",
    "gci -force '.' -ErrorAction SilentlyContinue | \n",
    "    ? { ($_ -is [io.directoryinfo]) } | % {\n",
    "        $len = 0\n",
    "        gci -recurse -force $_.fullname -ErrorAction SilentlyContinue | \n",
    "           % { $len += $_.length}\n",
    "        $_.fullname, ', {0:N2}' -f ($len / 1Mb)\n",
    "} | Out-File -FilePath './nytbbcdirs.csv' -Encoding UTF8\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = pd.read_csv(\"data/nytbbcdirs.csv\", encoding='utf-8', header=None, names=['dir', 'size'])\n",
    "folders = folders[folders['size'] > 19.57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\hunter\\\\dev\\\\data\\\\nytimes_bbc_2017_2020\\\\2019-05_new'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = folders.max()['dir'].rstrip(\" \"); path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The folder found above is the most complete archive of the first revisions of corrected articles.\n",
    "\n",
    "We will [recursively open all bottom-level subfiles](https://stackoverflow.com/a/13617120) as text in the two URL directories. The second line is always article title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('www.bbc.co.uk\\\\news\\\\in-pictures-48254948',\n",
       " 'www.nytimes.com\\\\2019\\\\05\\\\03\\\\us\\\\politics\\\\stop-robocalls-congress.html',\n",
       " 'www.bbc.co.uk\\\\news\\\\world-latin-america-48137781',\n",
       " 'www.bbc.co.uk\\\\news\\\\entertainment-arts-48181924',\n",
       " 'www.nytimes.com\\\\2019\\\\05\\\\17\\\\opinion\\\\ceo-pay-raises.html')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_set = set()\n",
    "for dir_, dirs, files in os.walk(path):\n",
    "    if '.git' in dirs:\n",
    "        dirs.remove('.git')\n",
    "    if '.gitignore' in files:\n",
    "        files.remove('.gitignore')\n",
    "    for file_name in files:\n",
    "        rel_dir = os.path.relpath(dir_, path)\n",
    "        rel_file = os.path.join(rel_dir, file_name)\n",
    "        file_set.add(rel_file)\n",
    "file_set = tuple(file_set); file_set[0:5]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles, texts = [], []\n",
    "\n",
    "for file in file_set:\n",
    "    with open(os.path.join(path, file), mode='r', encoding = 'utf-8', errors='ignore') as filein:\n",
    "        text = filein.readlines()\n",
    "        texts.append(\" \".join(text[2:]))\n",
    "        titles.append(text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = pd.DataFrame({'title':titles,\n",
    "                            'text':texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [\"url_ending_index\", \"from_reputable_source_index\",\n",
    "         \"today_index\", \"grammar_index\", \"quotation_index\",\n",
    "         \"past_tense_index\", \"present_tense_index\", \"should_index\",\n",
    "         \"opinion_index\", \"all_caps_index\", \"from_satire_source_index\",\n",
    "         \"exclamation_index\", \"apa_index\", \"name_source_index\",\n",
    "         \"interjection_index\", \"you_index\", \"dot_gov_ending_index\",\n",
    "         \"from_unreputable_source_index\"]\n",
    "try:\n",
    "    corrections_vectors = pd.read_csv('./data/correction_news_vectors.txt', delim_whitespace=True, header=None, names=feats)\n",
    "except FileNotFoundError:\n",
    "    corrections_vectors = []\n",
    "\n",
    "    for i, row in corrections.iterrows():\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"\\r\" + f\"{i!s}/{len(file_set)}\")\n",
    "        corrections_vectors.append(fe.ArticleVector(text=row['text'], title=row['title']).vector)\n",
    "\n",
    "    corrections_vectors = pd.DataFrame(corrections_vectors, columns=feats)\n",
    "    corrections_vectors = corrections_vectors.assign(url_ending_index=1)\n",
    "    corrections_vectors = corrections_vectors.assign(dot_gov_ending_index=0)\n",
    "\n",
    "    corrections_vectors.to_csv('./data/correction_news_vectors.txt', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_ending_index</th>\n",
       "      <th>from_reputable_source_index</th>\n",
       "      <th>today_index</th>\n",
       "      <th>grammar_index</th>\n",
       "      <th>quotation_index</th>\n",
       "      <th>past_tense_index</th>\n",
       "      <th>present_tense_index</th>\n",
       "      <th>should_index</th>\n",
       "      <th>opinion_index</th>\n",
       "      <th>all_caps_index</th>\n",
       "      <th>from_satire_source_index</th>\n",
       "      <th>exclamation_index</th>\n",
       "      <th>apa_index</th>\n",
       "      <th>name_source_index</th>\n",
       "      <th>interjection_index</th>\n",
       "      <th>you_index</th>\n",
       "      <th>dot_gov_ending_index</th>\n",
       "      <th>from_unreputable_source_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.074656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052063</td>\n",
       "      <td>0.075147</td>\n",
       "      <td>0.000982</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006876</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.162574</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.005403</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.099256</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008271</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.111663</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.063492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037698</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061181</td>\n",
       "      <td>0.072785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>1</td>\n",
       "      <td>0.113924</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.003165</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005063</td>\n",
       "      <td>0.035443</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.093671</td>\n",
       "      <td>0.043038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007595</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.230380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   url_ending_index  from_reputable_source_index  today_index  grammar_index  \\\n",
       "0                 1                            0     0.000491       0.074656   \n",
       "1                 1                            0     0.000000       0.061208   \n",
       "2                 1                            0     0.001984       0.063492   \n",
       "3                 1                            0     0.000000       0.040084   \n",
       "4                 1                            0     0.005063       0.035443   \n",
       "\n",
       "   quotation_index  past_tense_index  present_tense_index  should_index  \\\n",
       "0              0.0          0.052063             0.075147      0.000982   \n",
       "1              0.0          0.099256             0.064516      0.001654   \n",
       "2              0.0          0.041667             0.069444      0.000000   \n",
       "3              0.0          0.061181             0.072785      0.000000   \n",
       "4              0.0          0.093671             0.043038      0.000000   \n",
       "\n",
       "   opinion_index  all_caps_index  from_satire_source_index  exclamation_index  \\\n",
       "0              0        0.006876                         1           0.000000   \n",
       "1              0        0.008271                         1           0.000000   \n",
       "2              0        0.037698                         1           0.000000   \n",
       "3              0        0.004219                         1           0.001055   \n",
       "4              0        0.007595                         1           0.000000   \n",
       "\n",
       "   apa_index  name_source_index  interjection_index  you_index  \\\n",
       "0          1           0.162574            0.000491   0.005403   \n",
       "1          0           0.111663            0.000827   0.004136   \n",
       "2          0           0.253968            0.000000   0.007937   \n",
       "3          1           0.113924            0.001055   0.003165   \n",
       "4          0           0.230380            0.000000   0.000000   \n",
       "\n",
       "   dot_gov_ending_index  from_unreputable_source_index  \n",
       "0                     0                              0  \n",
       "1                     0                              0  \n",
       "2                     0                              0  \n",
       "3                     0                              0  \n",
       "4                     0                              0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrections_vectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
